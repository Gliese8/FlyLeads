{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "444d3fc0-7464-4271-bd8d-0c06c1c18361",
   "metadata": {},
   "source": [
    "# FlyLeads\n",
    "### Interpreting posts from Reddit and sourcing location and travel-restrictions features from GoogleMaps API and other dataset\n",
    "1. This Script accepts 1 parameters which is the key to access the project created on Google Cloud Console, namely the GoogleMaps API: **python3 process.py GOOGLE_PROJECT_KEY**\n",
    "2. Continues the ETL Process by: a) creating a control table and prepared table b) creating a mechanism that keeps tracked of what has been processed by date on the control table and storing the final results in the prepared table\n",
    "3. Processes 100 reddit-posts: identifies the destinations mentioned in the posts, adds geographical information and appends the travel-requirements per destination mentioned\n",
    "4. Stores the processed posts in the prepared table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "596055e9-53d7-4c9f-9761-9a9d3649ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spaCy\n",
    "#!python3 -m spacy download en_core_web_lg\n",
    "#!pip install matplotlib\n",
    "\n",
    "#!pip install -U googlemaps\n",
    "#!pip install seaborn\n",
    "#!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "520293dd-a129-4e35-91bf-f54b1063723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import googlemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "3620a17e-479d-4eda-a7fe-f83973505057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table proc_log if it does not exist\n",
    "def create_proc_log_table():\n",
    "    # connecting to the FlyLeads_user.db . If not existent, creates an empty db\n",
    "    conn = sqlite3.connect('FlyLeads_user.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # IF NOT EXISTS = only creates a table, if the proc_log table doesn't exist\n",
    "    c.execute(\n",
    "        \"CREATE TABLE IF NOT EXISTS proc_log(Post_ID TEXT, proc_time DATE)\")\n",
    "\n",
    "    # Closing the connection\n",
    "    c.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(\"proc_log table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9fc92cd9-8c93-45e1-a783-c1f8fbaa8dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table proc_posts if it does not exist\n",
    "def create_proc_posts_table():\n",
    "\n",
    "    # connecting to the FlyLeads_user.db. If not existent, creates an empty db\n",
    "    conn = sqlite3.connect('FlyLeads_user.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # IF NOT EXISTS = only creates a table, if the proc_posts table doesn't exist\n",
    "    c.execute(\n",
    "        \"CREATE TABLE IF NOT EXISTS proc_posts(Post_ID TEXT, URL TEXT, Destinations TEXT,\\\n",
    "         Latitude REAL, Longitude REAL, Country_Full_Name TEXT, Requirement TEXT)\")\n",
    "\n",
    "    # Closing the connection\n",
    "    c.close()\n",
    "    conn.close()\n",
    "    print(\"proc_posts table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f7ea5846-d867-4fea-b464-0120d0d4661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert in the control table the Post_IDs that are in the raw reddits table but not in the control table\n",
    "def insert_not_in_log():\n",
    "    conn = sqlite3.connect('FlyLeads_user.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    sql_query = ('INSERT INTO proc_log(Post_ID, proc_time)\\\n",
    "     SELECT raw_reddits.Post_ID, NULL\\\n",
    "     FROM raw_reddits\\\n",
    "     WHERE NOT EXISTS( SELECT 1 FROM proc_log WHERE raw_reddits.Post_ID = proc_log.Post_ID);')\n",
    "\n",
    "    c.execute(sql_query)\n",
    "    conn.commit()\n",
    "\n",
    "    c.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "7a86fa2d-5de8-4254-b5a7-915200ec1cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get df with  UP TO 100 records that exist in the control table but have an empty time field\n",
    "def query_empty_in_log():\n",
    "    conn = sqlite3.connect('FlyLeads_user.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    sql_query = \"SELECT proc_log.Post_ID, Title, Body, URL\\\n",
    "    FROM proc_log\\\n",
    "    LEFT JOIN raw_reddits on proc_log.Post_ID = raw_reddits.Post_ID\\\n",
    "    WHERE proc_log.proc_time is NULL\\\n",
    "    LIMIT 100\"\n",
    "\n",
    "    posts_df = pd.read_sql_query(sql_query, conn)\n",
    "\n",
    "    c.close()\n",
    "    conn.close()\n",
    "\n",
    "    return posts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dfea16-4bb1-41a6-a15e-06b78598feb0",
   "metadata": {},
   "source": [
    "#### Interpreting the posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "18a04512-0bfb-4a81-ba06-a728a5badea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds a column with countries to the df\n",
    "def extract_countries(row):\n",
    "    post_title = row['Title']\n",
    "    post_body = row['Body']\n",
    "\n",
    "    # Process the title and body of the post with spaCy to extract countries\n",
    "    doc_title = nlp(post_title)\n",
    "    doc_body = nlp(post_body)\n",
    "    countries = [ent.text for ent in doc_title.ents if ent.label_ == \"GPE\"] + [ent.text for ent in doc_body.ents if\n",
    "                                                                               ent.label_ == \"GPE\"]\n",
    "\n",
    "    row['Destinations'] = list(set([country.lower() for country in countries]))\n",
    "    return row\n",
    "\n",
    "def add_countries_column(df):\n",
    "    processed_df = df.apply(extract_countries, axis=1)\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9492a906-3050-45f7-be38-e65fdfde634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the processed df\n",
    "def get_subset_df(df):\n",
    "    subset_df = df[[\"Post_ID\", \"URL\", \"Destinations\"]].copy()\n",
    "    return subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c64603c0-6789-4020-a5df-058de79aac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand countries column in the subset df\n",
    "def get_expanded_df(subset_df):\n",
    "    expanded_rows = []\n",
    "\n",
    "    # Iterate over each row in the original DataFrame\n",
    "    for idx, row in subset_df.iterrows():\n",
    "        post_id = row['Post_ID']\n",
    "        url = row[\"URL\"]\n",
    "        destinations = row['Destinations']\n",
    "\n",
    "        # If destinations is a list, expand it into separate rows\n",
    "        if isinstance(destinations, list):\n",
    "            for destination in destinations:\n",
    "                expanded_rows.append({'Post_ID': post_id, 'URL': url, 'Destinations': destination})\n",
    "        # If destinations is a string, add it as a single row\n",
    "        else:\n",
    "            expanded_rows.append({'Post_ID': post_id, 'URL': url, 'Destinations': destinations})\n",
    "\n",
    "    # Create DataFrame from the list of expanded rows\n",
    "    expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "    return expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "18a0389d-612d-461e-bb90-ef297ca4b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all the process functions and returns processed df\n",
    "def process_df(df):\n",
    "    column_added_df = add_countries_column(df)\n",
    "    subset_df = get_subset_df(column_added_df)\n",
    "    expanded_df = get_expanded_df(subset_df)\n",
    "    print(\"initial process done.\")\n",
    "    return expanded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeaea1d-db23-411c-a220-24e2ae768833",
   "metadata": {},
   "source": [
    "#### Locations and Travel requirements\n",
    "**Let's use GoogleMaps API to**\n",
    "1. get the coordinates of a certain place from the place mentioned by the user\n",
    "2. use those coordinates to get the full country name of the place mentioned. This is important because people can mention nicknames of places or slang.\n",
    "3. with the country full names given by GoogleMaps we can associate them with the travel restrictions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eccfa92e-5db5-4a39-a41c-122429ac6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to geocode addresses and return latitude and longitude\n",
    "def geocode_address(address, gmaps):\n",
    "\n",
    "    try:\n",
    "        # Geocode the address\n",
    "        geocode_result = gmaps.geocode(address)\n",
    "        # Extract latitude and longitude\n",
    "        lat = geocode_result[0]['geometry']['location']['lat']\n",
    "        lng = geocode_result[0]['geometry']['location']['lng']\n",
    "        return lat, lng\n",
    "    except:\n",
    "        # If geocoding fails, return None\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ef57120-4515-41a4-88bc-6ce34842fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_geocode(lat, lng, gmaps):\n",
    "    try:\n",
    "        # Reverse geocode the coordinates\n",
    "        reverse_geocode_result = gmaps.reverse_geocode((lat, lng))\n",
    "        # Extract country from the result\n",
    "        country = None\n",
    "        for component in reverse_geocode_result[0]['address_components']:\n",
    "            if 'country' in component['types']:\n",
    "                country = component['long_name']\n",
    "                break\n",
    "        return country\n",
    "    except:\n",
    "        # If reverse geocoding fails, return None\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "0aae51ea-014a-4efe-a03b-bc3b7a6b37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding lat and long columns to our df\n",
    "def add_geo_columns(results_df, gmaps):\n",
    "    results_df['Latitude'], results_df['Longitude'] = zip(*results_df['Destinations'].\n",
    "                                                          apply(lambda x: geocode_address(x, gmaps)))\n",
    "    print(\"lat, lon cols added\")\n",
    "    results_df['Country_Full_Name'] = results_df.apply(lambda row: reverse_geocode(row['Latitude'],\n",
    "                                                                                   row['Longitude'], gmaps), axis=1)\n",
    "    print(\"country_name cols added\")\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fc1e27-2f19-49b4-8ab2-bb410462302d",
   "metadata": {},
   "source": [
    "**Passport Index 2024: visa requirements for 199 countries**\n",
    "\n",
    "We had to do some tranformations to the csv Travel visa requirements for 199 countries, in .csv from https://github.com/ilyankou/passport-index-dataset in order to get \n",
    "1. add a column to our results dataframe containing the possible visa requirements and its explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2ceccfc8-1539-42f9-bc62-b554e9387de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_requirements_p_country():\n",
    "    # Evaluate if it makes sense to read this table from the database\n",
    "    all_travel_requir_explan = pd.read_csv(\"all_travel_requirements_expl.csv\")\n",
    "\n",
    "    # Removing (Departure =) Passport column and Eliminating the duplicates\n",
    "    shortlist_requirements = all_travel_requir_explan.drop(labels=[\"Passport\"], axis=1).drop_duplicates().sort_values(\n",
    "        \"Destination\")\n",
    "\n",
    "    # Removing the -1 values becase it means they are from the same country\n",
    "    shortlist_requirements.drop(shortlist_requirements[shortlist_requirements['Requirement'] == '-1'].index,\n",
    "                                inplace=True)\n",
    "\n",
    "    # Grouping the Requirements on a list\n",
    "    grouped_requirements = shortlist_requirements.groupby('Destination')['Requirement'].agg(list).reset_index()\n",
    "    grouped_requirements[\"Requirement\"] = grouped_requirements[\"Requirement\"].apply(lambda x: ', '.join(x))\n",
    "\n",
    "    return grouped_requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "99f521f0-55df-401b-b64b-be65d1a12142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_requirements_column(results_df, grouped_requirements):\n",
    "    end_proc_df = results_df.merge(grouped_requirements, how=\"left\", left_on=\"Country_Full_Name\",\n",
    "                                  right_on=\"Destination\").drop(labels = [\"Destination\"], axis=1)\n",
    "\n",
    "    return end_proc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad701e3-f4ca-440d-bf9f-88ac3851622f",
   "metadata": {},
   "source": [
    "### Appending the results to proc_posts table and updating the process time on the control table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b6cdd-e36e-44a5-bf37-0f3a985edb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the df with new columns to proc_posts table (df to ddb)\n",
    "def append_proc_messages(messages_df):\n",
    "    # connecting to the db\n",
    "    conn = sqlite3.connect('FlyLeads_user.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Appends messages to the raw_messages table\n",
    "    messages_df.to_sql(\"proc_posts\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "    # closing connection\n",
    "    c.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51bf5e8-829e-4027-a465-e5304be8fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the NULL records of the column proc_time in proc_loc table with current timestamp\n",
    "def update_proc_time():\n",
    "    now = datetime.now()\n",
    "    date_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "    conn = sqlite3.connect('FlyLeads_user.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    sql_query = 'UPDATE proc_log SET proc_time = ? WHERE proc_time IS NULL'\n",
    "\n",
    "    c.execute(sql_query, (date_string,))\n",
    "    conn.commit()\n",
    "\n",
    "    c.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483b405-b119-4eb8-8318-0f66b583e347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    key = sys.argv[1]\n",
    "    gmaps = googlemaps.Client(key=key)\n",
    "    \n",
    "    # Create proc_log table in db if not existent\n",
    "    create_proc_log_table()\n",
    "    # Create proc_posts table in db if not existent\n",
    "    create_proc_posts_table()\n",
    "\n",
    "    # Insert in the control table the POST_IDs that are in the raw_reddits table but not in the control table\n",
    "    insert_not_in_log()\n",
    "\n",
    "    # Read up to 100 posts that exist in the control table but have an empty time field\n",
    "    posts_df = query_empty_in_log()\n",
    "\n",
    "    # Process posts and calculate new fields\n",
    "    results_df = process_df(posts_df)\n",
    "\n",
    "    # Adding geo columns\n",
    "    add_geo_columns(results_df, gmaps)\n",
    "\n",
    "    # getting list of requirements per country\n",
    "    grouped_requirements = get_requirements_p_country()\n",
    "\n",
    "    # Adding requirements list\n",
    "    end_proc_df = add_requirements_column(results_df, grouped_requirements)\n",
    "\n",
    "    # Insert them in the processed table proc_messages\n",
    "    append_proc_messages(end_proc_df)\n",
    "\n",
    "    # Update the proc_time field in the proc_log table\n",
    "    update_proc_time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
